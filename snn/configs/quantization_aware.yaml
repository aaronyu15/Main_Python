# Configuration for Quantization-Aware Training
# Progressive quantization: 32-bit -> 8-bit -> 4-bit -> 1-bit

# Model Configuration
model_type: 'SpikingFlowNet'
in_channels: 5
num_timesteps: 10
tau: 2.0
threshold: 1.0

# Quantization Settings - ENABLED
quantization_enabled: true
initial_bit_width: 32          # Start with full precision

# Quantization Schedule (epoch: bit_width)
# Progressive quantization helps model adapt gradually
quantization_schedule:
  0: 32      # Epochs 0-49: Full precision baseline
  50: 8      # Epochs 50-99: 8-bit quantization
  100: 4     # Epochs 100-149: 4-bit quantization
  150: 2     # Epochs 150+: 2-bit quantization (near-binary)

binarize: false                # Don't use binary mode (yet)

# Training Settings
num_epochs: 200
batch_size: 4
val_batch_size: 4
num_workers: 4

# Optimizer Settings
# Lower learning rate for quantization stability
learning_rate: 0.00005         # Lower than baseline
weight_decay: 0.0001
lr_milestones: [50, 100, 150]  # Align with quantization schedule
lr_gamma: 0.5

# Loss Weights
flow_weight: 1.0
smooth_weight: 0.1
sparsity_weight: 0.02          # Slightly higher for hardware efficiency
quant_weight: 0.001            # Higher weight for quantization regularization
target_spike_rate: 0.08        # Lower target for power efficiency

# Data Settings
data_root: '../blink_sim/output'  # Path to dataset root directory
use_events: true
crop_size: [256, 256]
max_train_samples: null
max_val_samples: null

# Training Loop Settings
grad_clip: 1.0
log_interval: 10
save_interval: 10

# Hardware Settings
target_hardware: 'FPGA'
target_power_budget: 'very_low'
