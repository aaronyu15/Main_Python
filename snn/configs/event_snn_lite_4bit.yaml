# EventSNNFlowNetLiteV2 - 4-bit Quantization Fine-tuning
# Fine-tune from 8-bit checkpoint with 4-bit quantization

# Model Configuration
model_type: 'EventSNNFlowNetLiteV2'
in_channels: 2
num_bins: 5
num_timesteps: 5
base_ch: 32
tau: 2.0
threshold: 1.0
alpha: 10.0
use_bn: false

# Quantization Settings - 4-bit QAT
quantization_enabled: true
initial_bit_width: 4
binarize: false
output_bit_width: 16                # Higher precision for output layer (flow prediction)
first_layer_bit_width: 8            # Keep first layer at 8-bit (higher than network 4-bit)
mem_bit_width: 16                   # Higher precision for membrane potential
hardware_mode: True

# Training Settings - Fine-tuning
num_epochs: 30
batch_size: 32
val_batch_size: 32
num_workers: 32

# Optimizer Settings - Even lower learning rate for aggressive quantization
learning_rate: 0.00002              # Lower for 4-bit stability
weight_decay: 0.0001
lr_milestones: [15, 25]
lr_gamma: 0.5

# Loss Weights
flow_weight: 1.0
smooth_weight: 0.1
sparsity_weight: 0.01
quant_weight: 0.002                 # Higher weight for 4-bit
target_spike_rate: 0.1

# Data Settings
data_root: '../train_set'
camera_size: [320, 320]
use_events: true
crop_size: [320, 320]
max_train_samples: null
max_val_samples: null

# Training Loop Settings
grad_clip: 1.0
log_interval: 10
save_interval: 5

# Hardware Settings
target_hardware: 'FPGA'
target_power_budget: 'low'
