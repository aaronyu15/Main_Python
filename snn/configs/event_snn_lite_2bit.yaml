# EventSNNFlowNetLite - 2-bit Quantization Fine-tuning
# Fine-tune from 4-bit checkpoint with 2-bit quantization

# Model Configuration
model_type: 'EventSNNFlowNetLite'
in_channels: 2
num_bins: 5
num_timesteps: 5
base_ch: 32
decay: 0.5                            # LIF neuron time constant
threshold: 16.0                      # LIF neuron firing threshold
alpha: 10.0

# Quantization Settings - 2-bit QAT
quantize_weights: true              # Enable weight quantization
quantize_activations: true          # Enable activation quantization
quantize_mem: true                  # Enable membrane potential quantization
weight_bit_width: 2                 # Weight quantization bit-width
act_bit_width: 4                    # Activation quantization bit-width
output_bit_width: 16                # Higher precision for output layer (flow prediction)
first_layer_bit_width: 8            # Keep first layer at 8-bit (much higher than network 2-bit)
mem_bit_width: 16                   # Higher precision for membrane potential
log_params: true                    # Log model parameters and statistics to TensorBoard

# Training Settings - Fine-tuning
num_epochs: 4                      # More epochs for extreme quantization
batch_size: 4
val_batch_size: 4
num_workers: 8

# Optimizer Settings - Very low learning rate for extreme quantization
learning_rate: 0.00001
weight_decay: 0.0001
lr_milestones: [20, 32]
lr_gamma: 0.5

# Loss Weights
flow_weight: 1.0
angular_weight: 0.0              # Angular error loss weight (set >0 to enable)
smooth_weight: 0.1

# Data Settings
data_root: '../blink_sim/output/train'
#data_root: '../train_set'
camera_size: [320, 320]
use_events: true
crop_size: [320, 320]
max_train_samples: null
max_val_samples: null

# Training Loop Settings
grad_clip: 0.5                      # Tighter clipping for stability
log_interval: 10
save_interval: 5
