# Configuration for Standard SNN Training (Full Precision)
# Good starting point for initial training

# Model Configuration
model_type: 'SpikingFlowNet'  # or 'SpikingFlowNetLite' for lightweight version
in_channels: 5                 # Number of event voxel bins
num_timesteps: 10              # Number of SNN simulation timesteps
tau: 2.0                       # LIF neuron time constant
threshold: 1.0                 # LIF neuron firing threshold

# Quantization Settings
quantization_enabled: false    # Enable quantization-aware training
initial_bit_width: 32          # Initial bit width (32 = full precision)
binarize: false                # Binary weights and activations

# Training Settings
num_epochs: 200
batch_size: 4
val_batch_size: 4
num_workers: 4

# Optimizer Settings
learning_rate: 0.0001
weight_decay: 0.0001
lr_milestones: [100, 150]      # Epochs to reduce learning rate
lr_gamma: 0.5                  # Learning rate reduction factor

# Loss Weights
flow_weight: 1.0               # Main flow loss weight
smooth_weight: 0.1             # Smoothness regularization
sparsity_weight: 0.01          # Spike sparsity loss (for SNN efficiency)
quant_weight: 0.0001           # Quantization regularization
target_spike_rate: 0.1         # Target spike rate (10% of neurons firing)

# Data Settings
data_root: '../blink_sim/output'  # Path to dataset root directory
use_events: true               # Use event data (vs RGB images)
crop_size: [256, 256]          # Image crop size
max_train_samples: null        # Limit training samples (null = use all)
max_val_samples: null          # Limit validation samples

# Training Loop Settings
grad_clip: 1.0                 # Gradient clipping threshold
log_interval: 10               # Log every N batches
save_interval: 10              # Save checkpoint every N epochs

# Hardware Settings
# These settings don't affect training but document target hardware
target_hardware: 'FPGA'
target_power_budget: 'low'     # Aim for low power consumption
